import os
import pandas as pd
import io 
from azure.identity import DefaultAzureCredential
from azure.storage.filedatalake import DataLakeServiceClient
from include.logs.logger import setup_logger
from dotenv import load_dotenv

load_dotenv()
logger = setup_logger(__name__)

class AzureDataLakeClient:
    def __init__(self):
        # 1. L·∫•y Connection String tr·ª±c ti·∫øp t·ª´ m√¥i tr∆∞·ªùng
        self.connection_string = os.getenv("AZURE_CONNECTION_STRING")
        self.file_system_name = os.getenv("AZURE_CONTAINER_NAME", "vietnamworks")
        
        if not self.connection_string:
            logger.error("Missing AZURE_CONNECTION_STRING in .env file.")
            raise ValueError("AZURE_CONNECTION_STRING is not set.")

        logger.info(f"Initializing ADLS Gen2 Client via Connection String for container: {self.file_system_name}")
        
        try:
            # 2. Kh·ªüi t·∫°o client chu·∫©n x√°c qua h√†m from_connection_string
            self.service_client = DataLakeServiceClient.from_connection_string(self.connection_string)
            self.file_system_client = self.service_client.get_file_system_client(self.file_system_name)
        except Exception as e:
            logger.error(f"Failed to initialize ADLS Gen2 client: {e}")
            raise

    def close(self):
        """ƒê√≥ng socket pool khi kh√¥ng d√πng n·ªØa"""
        self.service_client.close()

    def upload_dataframe(self, df: pd.DataFrame, remote_path: str, format='parquet'):
        """
        Upload DataFrame tr·ª±c ti·∫øp l√™n ADLS Gen2 (Kh√¥ng l∆∞u file t·∫°m)
        :param df: Pandas DataFrame
        :param remote_path: ƒê∆∞·ªùng d·∫´n tr√™n Data Lake (VD: silver/jobs.parquet)
        :param format: 'parquet' (khuy√™n d√πng) ho·∫∑c 'csv'
        """
        try:
            # 1. T·∫°o buffer trong RAM (IO Stream)
            output_buffer = io.BytesIO()

            # 2. Ghi DataFrame v√†o buffer (Serialize)
            if format == 'parquet':
                # Parquet n√©n t·ªët, gi·ªØ ƒë∆∞·ª£c schema (int l√† int, string l√† string)
                df.to_parquet(output_buffer, index=False)
            elif format == 'csv':
                # CSV ch·ªâ d√πng cho debug, kh√¥ng n√™n d√πng cho pipeline ch√≠nh
                df.to_csv(output_buffer, index=False, encoding='utf-8')
            else:
                logger.error(f"Unsupported format: {format}")
                raise ValueError("Format not supported. Use 'parquet' or 'csv'.")

            # 3. ƒê∆∞a con tr·ªè v·ªÅ ƒë·∫ßu buffer ƒë·ªÉ chu·∫©n b·ªã ƒë·ªçc
            data = output_buffer.getvalue()

            # 4. Upload byte stream l√™n Azure
            file_client = self.file_system_client.get_file_client(remote_path)
            file_client.upload_data(data, overwrite=True)
            
            logger.info(f"Uploaded DataFrame to {remote_path} ({len(data)/1024:.2f} KB)")

        except Exception as e:
            logger.error(f"Error uploading DataFrame: {e}")
            raise

    def get_dataframe(self, remote_path: str, format='parquet') -> pd.DataFrame:
        """
        ƒê·ªçc tr·ª±c ti·∫øp t·ª´ Azure m√† kh√¥ng c·∫ßn t·∫£i buffer th·ªß c√¥ng.
        T·∫≠n d·ª•ng t·ªëi ƒëa kh·∫£ nƒÉng Streaming v√† Predicate Pushdown c·ªßa Parquet.
        """
        # C·∫•u tr√∫c URL chu·∫©n c·ªßa ADLS Gen2 cho Pandas
        # abfs://<container>/<folder>/<file>
        full_path = f"abfs://{self.file_system_name}/{remote_path}"
        
        # Truy·ªÅn credential v√†o storage_options
        # adlfs s·∫Ω t·ª± d√πng DefaultAzureCredential n·∫øu kh√¥ng truy·ªÅn key, 
        # nh∆∞ng truy·ªÅn account_name l√† b·∫Øt bu·ªôc.
        storage_options = {
            "connection_string": self.connection_string
        }

        print(f"üîÑ Streaming DataFrame from {full_path}...")

        try:
            if format == 'parquet':
                # Parquet th√¥ng minh: N√≥ ch·ªâ t·∫£i ph·∫ßn Footer v√† Metadata tr∆∞·ªõc,
                # Sau ƒë√≥ ch·ªâ t·∫£i c√°c c·ªôt c·∫ßn thi·∫øt (Columnar Storage).
                # => Ti·∫øt ki·ªám RAM c·ª±c l·ªõn.
                df = pd.read_parquet(full_path, storage_options=storage_options)
                
            elif format == 'csv':
                # CSV v·∫´n ph·∫£i ƒë·ªçc tu·∫ßn t·ª±, nh∆∞ng adlfs qu·∫£n l√Ω buffer t·ªët h∆°n 
                # vi·ªác b·∫°n readall()
                df = pd.read_csv(full_path, storage_options=storage_options)
                
            else:
                raise ValueError(f"Unsupported format: {format}")

            print(f"‚úÖ Loaded {len(df)} rows.")
            return df

        except Exception as e:
            print(f"‚ùå Error reading dataframe: {e}")
            raise