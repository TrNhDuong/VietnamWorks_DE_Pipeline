import os
import pandas as pd
import io 
from azure.identity import DefaultAzureCredential
from azure.storage.filedatalake import DataLakeServiceClient
from include.logs.logger import setup_logger
from dotenv import load_dotenv

load_dotenv()
logger = setup_logger(__name__)

class AzureDataLakeClient:
    def __init__(self):
        # S·ª≠ d·ª•ng DefaultAzureCredential ƒë·ªÉ t·ª± ƒë·ªông d√≤ t√¨m credential
        # (∆Øu ti√™n: Environment Var -> Workload Identity -> Managed Identity -> Azure CLI -> Interactive)
        self.credential = DefaultAzureCredential()
        
        self.account_name = os.getenv("AZURE_STORAGE_ACCOUNT_NAME")
        self.file_system_name = os.getenv("AZURE_CONTAINER_NAME", "vietnamworks") # T∆∞∆°ng ƒë∆∞∆°ng Bucket trong MinIO
        
        self.account_url = f"https://{self.account_name}.dfs.core.windows.net"
        
        logger.info(f"Initializing Azure Data Lake Client for account: {self.account_name}, container: {self.file_system_name}")
        try:
            self.service_client = DataLakeServiceClient(
                account_url=self.account_url, 
                credential=self.credential
            )
            self.file_system_client = self.service_client.get_file_system_client(self.file_system_name)
        except Exception as e:
            logger.error(f"Failed to initialize ADLS Gen2 client: {e}")
            raise

    def close(self):
        """ƒê√≥ng socket pool khi kh√¥ng d√πng n·ªØa"""
        self.service_client.close()

    def upload_dataframe(self, df: pd.DataFrame, remote_path: str, format='parquet'):
        """
        Upload DataFrame tr·ª±c ti·∫øp l√™n ADLS Gen2 (Kh√¥ng l∆∞u file t·∫°m)
        :param df: Pandas DataFrame
        :param remote_path: ƒê∆∞·ªùng d·∫´n tr√™n Data Lake (VD: silver/jobs.parquet)
        :param format: 'parquet' (khuy√™n d√πng) ho·∫∑c 'csv'
        """
        try:
            # 1. T·∫°o buffer trong RAM (IO Stream)
            output_buffer = io.BytesIO()

            # 2. Ghi DataFrame v√†o buffer (Serialize)
            if format == 'parquet':
                # Parquet n√©n t·ªët, gi·ªØ ƒë∆∞·ª£c schema (int l√† int, string l√† string)
                df.to_parquet(output_buffer, index=False)
            elif format == 'csv':
                # CSV ch·ªâ d√πng cho debug, kh√¥ng n√™n d√πng cho pipeline ch√≠nh
                df.to_csv(output_buffer, index=False, encoding='utf-8')
            else:
                logger.error(f"Unsupported format: {format}")
                raise ValueError("Format not supported. Use 'parquet' or 'csv'.")

            # 3. ƒê∆∞a con tr·ªè v·ªÅ ƒë·∫ßu buffer ƒë·ªÉ chu·∫©n b·ªã ƒë·ªçc
            data = output_buffer.getvalue()

            # 4. Upload byte stream l√™n Azure
            file_client = self.file_system_client.get_file_client(remote_path)
            file_client.upload_data(data, overwrite=True)
            
            logger.info(f"Uploaded DataFrame to {remote_path} ({len(data)/1024:.2f} KB)")

        except Exception as e:
            logger.error(f"Error uploading DataFrame: {e}")
            raise

    def get_dataframe(self, remote_path: str, format='parquet') -> pd.DataFrame:
        """
        ƒê·ªçc tr·ª±c ti·∫øp t·ª´ Azure m√† kh√¥ng c·∫ßn t·∫£i buffer th·ªß c√¥ng.
        T·∫≠n d·ª•ng t·ªëi ƒëa kh·∫£ nƒÉng Streaming v√† Predicate Pushdown c·ªßa Parquet.
        """
        # C·∫•u tr√∫c URL chu·∫©n c·ªßa ADLS Gen2 cho Pandas
        # abfs://<container>/<folder>/<file>
        full_path = f"abfs://{self.file_system_name}/{remote_path}"
        
        # Truy·ªÅn credential v√†o storage_options
        # adlfs s·∫Ω t·ª± d√πng DefaultAzureCredential n·∫øu kh√¥ng truy·ªÅn key, 
        # nh∆∞ng truy·ªÅn account_name l√† b·∫Øt bu·ªôc.
        storage_options = {
            "account_name": self.account_name,
            "anon": False # B·∫Øt bu·ªôc ph·∫£i x√°c th·ª±c
            # N·∫øu b·∫°n d√πng DefaultAzureCredential th√¨ kh√¥ng c·∫ßn 'account_key'
        }

        print(f"üîÑ Streaming DataFrame from {full_path}...")

        try:
            if format == 'parquet':
                # Parquet th√¥ng minh: N√≥ ch·ªâ t·∫£i ph·∫ßn Footer v√† Metadata tr∆∞·ªõc,
                # Sau ƒë√≥ ch·ªâ t·∫£i c√°c c·ªôt c·∫ßn thi·∫øt (Columnar Storage).
                # => Ti·∫øt ki·ªám RAM c·ª±c l·ªõn.
                df = pd.read_parquet(full_path, storage_options=storage_options)
                
            elif format == 'csv':
                # CSV v·∫´n ph·∫£i ƒë·ªçc tu·∫ßn t·ª±, nh∆∞ng adlfs qu·∫£n l√Ω buffer t·ªët h∆°n 
                # vi·ªác b·∫°n readall()
                df = pd.read_csv(full_path, storage_options=storage_options)
                
            else:
                raise ValueError(f"Unsupported format: {format}")

            print(f"‚úÖ Loaded {len(df)} rows.")
            return df

        except Exception as e:
            print(f"‚ùå Error reading dataframe: {e}")
            raise